---
title: "Practical Machine Learning Project"
author: "freestander"
date: "Sunday, September 27, 2015"
output: pdf_document
---

## Introduction

This project analyzes the personal activity data collected from body movement. The purpose is to quantify how well people perform on a particular activity by using a predictive model to classify correct and incorrect movements.


## Data Sources

The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

More information is available from the website here: 
http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Data Processing 

First we download the data from sources and load it into R studio.

```{r}
# remove all the previous data in the environment
rm(list = ls(all = TRUE))

# set up the working directory
setwd("C://Users//Qi-Desktop//Documents//RProjects//Coursera//Practical_Machine_Learning")

# load the data set for training and testing
training <- read.csv(file="./Data/pml-training.csv", header=TRUE, as.is = TRUE, stringsAsFactors = FALSE, sep=',', na.strings=c("", "NA", "#DIV/0!"))
testing_ds <- read.csv(file="./Data/pml-testing.csv", header=TRUE, as.is = TRUE, stringsAsFactors = FALSE, sep=',', na.strings=c("", "NA", "#DIV/0!"))

# convert the target variable into factor variable
training$classe <- as.factor(training$classe)
```

First we check the dimension of the training data set. The original training data contains 19622 records and 160 variables.

```{r}
dim(training)
```

Next we check the dimension of the testing data set. The testing data contains 20 records and 160 variables.

```{r}
dim(testing_ds)
```

Next we do some data processing to prepare for the modeling dataset. First we load "caret" package and split the training data into training set (70%) and validation set (30%).

```{r, echo=FALSE}
library(caret)
# split the training data set into training (70%) and validation (30%) subsets
set.seed(123)
train_split <- createDataPartition(training$classe, p = 0.7, list = FALSE)
training_ds <- training[train_split, ]
validation_ds <- training[-train_split, ]
```

## Variable Selection

By inspecting the variables in the data sets, we can see a lot variables with NA values or near zero variance. Also some descriptive variables should not be included in the prediction model.

```{r}
# function to check the number of missing values from a variable
na_size <- sapply(training_ds, function(x) 
  {
  sum(is.na(x))
  })
# variables with more than 90% missing values
na_var <- names(na_size[na_size >= 0.9 * dim(training_ds)[1]])
# exclude variables with more than 90% missing values
training_ds <- training_ds[, !names(training_ds) %in% na_var]

# variables with descriptive features 
descr_var <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
# exclude variables with descriptive features 
training_ds <- training_ds[, !names(training_ds) %in% descr_var]

# variables near zero variance
no_variance_var <- nearZeroVar(training_ds)
# exclude variables with near zero variance
training_ds <- training_ds[, !names(training_ds) %in% no_variance_var]
```

We can check the dimension of the training set after the split and variable selection. The training set now only contains 13737 records and 53 variables.

```{r}
dim(training_ds)
```

By visualizing the distribution of the target variable, we found no rare "classe" that needs special processing, e.g., stratified sampling or weighted regression.
```{r}
# frequency table of the target variable
table(training_ds$classe)
# frequency plot of the target variable
plot(training_ds$classe, main="Distribution of classe in the Training Set", xlab="classe Levels", ylab="Frequency")
```

## Predictive Model Training

We are going to train the predictive model using two popular algorithms "rpart" (decision trees) and "rf" (random forest). 

First we train the model using "rpart" algorithm, and use confusion matrix to check prediction accurancy. As from the output below, the prediction accuracy is 74.03%.

```{r, cache=TRUE}
set.seed(123)
library(rpart)
model_rpart <- rpart(classe ~ ., data=training_ds, method="class")
# use confusion matrix to check accuracy
rpart_pred_traiing_ds <- predict(model_rpart, training_ds, type="class")
confusionMatrix(training_ds$classe, rpart_pred_traiing_ds)
```

Next we training the model using "rf" algorithm, and use confusion matrix to check prediction accurancy. As from the output below, the prediction accuracy is 100%. Note than we only use 10 trees to avoid excessive computational time.

```{r, cache=TRUE}
set.seed(123)
# train the model using "rf" algorithm
# model_rf <- train(classe ~ ., method="rf", data=training_ds, verbose=F)
library(randomForest)
model_rf <- randomForest(classe ~ ., data=training_ds, importance=TRUE, ntrees=10)
# use confusion matrix to check accuracy
rf_pred_training_ds <- predict(model_rf, training_ds)
confusionMatrix(training_ds$classe, rf_pred_training_ds)
```

## Model Selection

Prediction result from the training set is not enough to tell which model is better since the model may be overfitting. Next We check the prediction results of the two models using the validation set.

First we check prediction accurancy on the validation set using "rpart" algorithm. As from the output below, the prediction accuracy is 73.15%, smaller than 74.03% from the training set.

```{r}
# use confusion matrix to check accuracy
library(rpart)
rpart_pred_validation_ds <- predict(model_rpart, validation_ds, type="class")
confusionMatrix(validation_ds$classe, rpart_pred_validation_ds)
```

Next we check prediction accurancy on the validation set using "rf" algorithm. As from the output below, the prediction accuracy is 99.46%, smaller than 100% from the training set.

```{r}
# use confusion matrix to check accuracy
library(randomForest)
rf_pred_validation_ds <- predict(model_rf, validation_ds)
confusionMatrix(validation_ds$classe, rf_pred_validation_ds)
```

As a result, "rf" model has better prediction accuracy on the validation set, and we will use it to predict on the final testing set.

## Test Set Prediction Result

Finally, we use "rf" model to predict on the final testing set.

```{r, echo=FALSE}
library(randomForest)
answers <- predict(model_rf, testing_ds)
answers
```

We then save the output to text files according to instructions and post it to the submission page.

```{r, echo=FALSE}
# convert result to a character vector
answers <- as.vector(answers)

pml_write_files = function(x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("problem_id_", i, ".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)
    }
}

pml_write_files(answers)
```





